This section will discuss how we evaluated that our system was functioning according to our requirements. Because our project had a focus on technical issues, we did not conduct any formal usability evaluations with potential users. We did not conduct any formal user evaluations as the would only serve to gather data about the usability of our interface, which given time constraints, was not a huge priority. However, we did conduct many informal testing sessions under normal conditions (with many users logged on), to get some feedback about our interface and features, in addition to testing our system - but we did not collect any data on such feedback. 

Instead, our evaluations were more focused on ensuring our system complied to the specifications we have drawn out in section \ref{features}. As our system is modular, we decided a sensible boundary for our test harnesses would be around the client and the server. Our goal in testing was focused on ensuring the client and server performed their roles as outlined in the GIM protocol, and to attempt to ensure that our our threading was robust.  These aspects, and an analysis of our testing methods, will be outlined in sections \ref{server_eval} and \ref{client_eval}. Much of our testing was conducted using volunteers who used the client from its earliest versions, to later versions. Testing with users naturally allowed us to test threading issues, as when many users are logged there is a greater degree of synchronization. The server receives commands from more users simultaneously, and the client receives more commands from the server about events it needs to know about.
