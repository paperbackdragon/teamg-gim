This section will discuss how we evaluated that our system was functioning according to our requirements. Because our project had a focus on technical issues, we did not conduct any formal usability evaluations with potential users. This was because they would only serve to gather data about the usability of our interface, which was not a huge priority given our time constraints. However, we did conduct many informal testing sessions under normal conditions (with many users logged in), to get some feedback about our interface and features in addition to testing our system.

Our evaluations were more focused on ensuring our system complied to the specifications we drew out in section \ref{features}. As our system is modular, we decided a sensible boundary for our test harnesses would be around the client and the server. Our goal in testing was focused on ensuring the client and server performed their roles as outlined in the GIM protocol, and to attempt to ensure that our our threading was robust. These aspects, and an analysis of our testing methods, will be outlined in sections \ref{server_eval} and \ref{client_eval}. Much of our testing was conducted using volunteers who used the client from its earliest versions to later versions. Testing with users naturally allowed us to test threading issues, as when many users are logged there is a greater degree of synchronization. The server receives commands from more users simultaneously, and the clients receive more commands from the server about events relevant to them.
